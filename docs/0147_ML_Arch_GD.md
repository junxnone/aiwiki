---
Title | ML Arch GD
-- | --
Created @ | `2018-12-12T10:16:49Z`
Updated @| `2023-02-02T09:31:49Z`
Labels | ``
Edit @| [here](https://github.com/junxnone/aiwiki/issues/147)

---
# Gradient Descent 梯度下降

## Reference

- [随机梯度下降法，批量梯度下降法和小批量梯度下降法以及代码实现](https://blog.csdn.net/LoseInVain/article/details/78243051)
- [优化器（Optimizer）介绍](https://blog.csdn.net/weixin_41417982/article/details/81561210)
- [为什么说随机最速下降法(SGD)是一个很好的方法？](https://zhuanlan.zhihu.com/p/27609238)
- [SGD过程中的噪声如何帮助避免局部极小值和鞍点？](https://zhuanlan.zhihu.com/p/36816689) 
- [Deep Learning 最优化方法之SGD](https://blog.csdn.net/bvl10101111/article/details/72615436)
- [最清晰的讲解各种梯度下降法原理与Dropout](https://baijiahao.baidu.com/s?id=1613121229156499765&wfr=spider&for=pc)
- [一文搞懂深度学习中的梯度下降](https://www.cnblogs.com/wangguchangqing/p/10521330.html)
- [An overview of gradient descent optimization algorithms](https://arxiv.org/pdf/1609.04747.pdf)
- [梯度下降优化算法概述](https://alanlee.fun/2017/10/08/gradient-descent-methods/)
- [paper - Learning internal representations by error propagation.pdf](https://github.com/junxnone/AI/files/4431755/Learning.internal.representations.by.error.propagation.pdf)
- [gradient descent visualization tool](https://github.com/lilipads/gradient_descent_viz)
- [deeplearningbook  - optimization](https://www.deeplearningbook.org/contents/optimization.html)

## Brief
- [Gradient Descent 原理推导](/Gradient_Descent_原理推导)
- [BGD/SGD/MBGD](/Gradient_Descent_BGD_SGD_MBGD)
- [Gradient Descent History](/Gradient_Descent_History)
- [Gradient Descent Momentum/Nesterov](/Gradient_Descent_Momentum)

## 对比

![1](https://user-images.githubusercontent.com/2216970/54171780-98cfc280-44b6-11e9-9588-ef8249c67586.gif) | ![2](https://user-images.githubusercontent.com/2216970/54171786-9bcab300-44b6-11e9-9e80-3550a56ed548.gif)
-- | --



