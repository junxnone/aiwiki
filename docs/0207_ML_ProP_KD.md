---
Title | ML ProP KD
-- | --
Created @ | `2021-04-12T02:03:32Z`
Updated @| `2023-02-02T09:26:14Z`
Labels | ``
Edit @| [here](https://github.com/junxnone/aiwiki/issues/207)

---
# Knowledge Distillation 知识蒸馏

## Reference
- 2020-6 Knowledge Distillation: A Survey [[Paper](https://arxiv.org/pdf/2006.05525.pdf)]
- **Awesome list**
  - **[Awesome Knowledge Distillation](https://github.com/dkozlov/awesome-knowledge-distillation)**
  - [Awesome Knowledge-Distillation](https://github.com/FLHonker/Awesome-Knowledge-Distillation)

## Brief
- Teacher/Student Model
  - Teacher Model 的输出 作为 Soft Target Training  Student Model
  - Student 学习 Teacher 泛化能力
- 用途
  - 模型压缩(小模型学习大模型的泛化能力)
- [History](https://github.com/junxnone/aiwiki/issues/336)
- [Distilling the Knowledge in a Neural Network](/Distilling_the_Knowledge_in_a_Neural_Network)
---

![image](https://user-images.githubusercontent.com/2216970/114331485-a9ac0000-9b76-11eb-8d73-58a586283fef.png)


## Trend


