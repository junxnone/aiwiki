---
Title | Knowledge Distillation
-- | --
Create Date | `2021-04-12T02:03:32Z`
Update Date | `2022-01-19T11:08:11Z`
Edit link | [here](https://github.com/junxnone/aiwiki/issues/207)

---
## Reference
- 2020-6 Knowledge Distillation: A Survey [[Paper](https://arxiv.org/pdf/2006.05525.pdf)]
- **Awesome list**
  - **[Awesome Knowledge Distillation](https://github.com/dkozlov/awesome-knowledge-distillation)**
  - [Awesome Knowledge-Distillation](https://github.com/FLHonker/Awesome-Knowledge-Distillation)

## Brief
- Teacher/Student Model
  - Teacher Model 的输出 作为 Soft Target Training  Student Model
  - Student 学习 Teacher 泛化能力
- 用途
  - 模型压缩(小模型学习大模型的泛化能力)
- [History](https://github.com/junxnone/tech-io/issues/961)
---

![image](https://user-images.githubusercontent.com/2216970/114331485-a9ac0000-9b76-11eb-8d73-58a586283fef.png)


## Trend


