-----

| Title     | ML Attention Bahdanau                                 |
| --------- | ----------------------------------------------------- |
| Created @ | `2020-07-01T03:51:29Z`                                |
| Updated @ | `2025-02-08T01:34:23Z`                                |
| Labels    | \`\`                                                  |
| Edit @    | [here](https://github.com/junxnone/aiwiki/issues/214) |

-----

# Bahdanau Attention

| Pipeline                                                     | 公式                                                                                                                                                                                                                                                                |
| ------------------------------------------------------------ | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| ![image](media/99df4acd093fda1a469892e94a4aad6a2dd862d6.png) | ![image](media/ee078dba1323f0adf2b2fd48052828f73a6dcea5.png) <br>![image](media/f1d08467350782713d03df514e45a0555ce36a99.png) <br> ![image](media/e4f287752cff75a59dc37ef7cb8ee2c12c7f7fe7.png) <br> ![image](media/0d03900125aeed84fccb4e0952d9ac4ac70cd93f.png) |

## Reference

  - [paper - 2014 - Neural Machine Translation by Jointly Learning to
    Align and Translate](https://arxiv.org/abs/1409.0473)
  - [paper - 2015 - Effective Approaches to Attention-based Neural
    Machine Translation](https://arxiv.org/abs/1508.04025)
  - [BahdanauAttention与LuongAttention注意力机制简介](https://blog.csdn.net/u010960155/article/details/82853632)
  - [一文看懂 Bahdanau 和 Luong 两种 Attention
    机制的区别](https://zhuanlan.zhihu.com/p/129316415)
