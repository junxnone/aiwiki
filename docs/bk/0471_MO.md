-----

| Title     | MO                                                    |
| --------- | ----------------------------------------------------- |
| Created @ | `2024-06-28T15:23:38Z`                                |
| Updated @ | `2024-06-28T15:23:38Z`                                |
| Labels    | \`\`                                                  |
| Edit @    | [here](https://github.com/junxnone/aiwiki/issues/471) |

-----

# 模型性能优化技术

  - 量化 - Quantization
  - 稀疏 - Pruning/Sparsity
  - 蒸馏 - Distillation

## 量化

  - Post Training Quantization
      - 静态量化
      - 动态量化
      - 仅权重量化(LLM)
  - During training quantization(训练过程中模拟量化过程)
      - Quantization-aware training
  - Mixed-Precision
      - 引入不同数据类型

### 静态量化

  - 不需要 Fine-tune
  - 在 Calibration Dataset 上做前向推理，收集 Tensor 分布，计算可以量化的层的 min/max, 获取 zero
    point 和 scale , FP32-\> INT8/INT4
  - 运行在 INT8 上
  - 输入和权重 都量化
  - One scale for weight
  - Per output channel scale for weight

### 动态量化

  - 不需要数据调校
  - 根据 mini-batch 统计数据分布，获取 min/max -\> zero point/scale，实时转换 tensor
    FP32 -\> INT8

### 仅权重量化

  - 压缩到 INT4 ，计算式转到 FP32
  - 放在 Cache 中，节省 Memory 移动时间 \[Cacheline 时间\]

## 非对称量化和对称量化

  - Asymmetric quantization - 通常用于输入
  - Symmetric quantization - 通常用于权重

## Reference

  - [模型优化技术概览](https://www.bilibili.com/video/BV1qy411q71x/)
