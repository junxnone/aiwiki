-----

| Title     | paper AnomalyAny cn                                   |
| --------- | ----------------------------------------------------- |
| Created @ | `2025-06-03T02:04:27Z`                                |
| Updated @ | `2025-06-03T03:01:59Z`                                |
| Labels    | \`\`                                                  |
| Edit @    | [here](https://github.com/junxnone/aiwiki/issues/520) |

-----

# AnomalyAny CN

  - 2024.6 **Anomaly Anything: Promptable Unseen Visual Anomaly
    Generation**

<!-- end list -->

``` 
Han Sun  EPFL, Switzerland  Yunkang Cao   HUST, China   Hao Dong  ETH Zurich, Switzerland  Olga Fink  EPFL, Switzerland

{han.sun, olga.fink}@epfl.ch, cyk hust@hust.edu.cn, hao.dong@ibk.baug.ethz.ch

```

## Abstract

由于异常数据样本的稀缺，视觉异常检测（AD）面临着巨大挑战。尽管已有许多工作致力于合成异常样本，但这些合成的异常往往缺乏真实性，或者需要大量的训练数据，这限制了它们在现实场景中的应用。在这项工作中，我们提出了Anomaly
Anything（AnomalyAny），这是一个新颖的框架，它利用Stable
Diffusion（SD）的图像生成能力来生成多样且逼真的未知异常。通过在测试时以单个正常样本为条件，AnomalyAny能够针对任意物体类型，结合文本描述生成未知的异常。在AnomalyAny框架内，我们**提出了注意力引导的异常优化方法，以引导SD将注意力集中在生成关键的异常概念上**。此外，我们还**引入了提示引导的异常细化方法，通过融入详细的描述来进一步提升生成质量**。在MVTec
AD和VisA数据集上进行的大量实验表明，AnomalyAny具备生成高质量未知异常的能力，并且在提升下游异常检测任务的性能方面效果显著。我们的演示和代码可在<https://hansunhayden.github.io/AnomalyAny.github.io/>
获取。

## 1 引言

![Image](media/5fa83f25b06e89138d5c6e1cfb4cc2d341d072e5.png)

**图 1. 不同视觉异常生成方法的对比。与现有方法相比，AnomalyAny 无需训练即可生成多样且逼真的未知异常。**

视觉异常检测（AD）\[5\]旨在识别图像数据中偏离特征正态分布的不寻常或意外模式，这在工业检测和质量控制等领域至关重要\[3\]。由于异常样本罕见且难以收集，大多数现有AD方法依赖仅使用正常样本的无监督学习\[12,22,32\]。尽管AD领域最近取得了进展，但用于训练的异常样本稀缺仍是一个长期存在的挑战。为解决这一问题，各种研究已探索视觉异常生成，如图1所示：一些方法\[33,35\]通过从其他数据集中的自然图案或图像本身裁剪粘贴随机图案来增强正常样本，虽能生成多样化异常样本且无需训练即可应用于未知物体，但样本真实性不足；另一种利用生成模型\[17\]的方法虽能产出更逼真图像，却需要充足且具代表性的正常与异常样本进行训练——这对AD任务而言通常难以实现。

鉴于异常的稀有性和可变性，收集代表性异常样本本身就极为困难，而工业场景中产品变体与配置的多样性更导致代表性正常样本匮乏\[18\]，双重限制加剧了两类样本的短缺。这使得生成模型在数据有限的现实应用中适用性较差，且易受少量训练样本的偏差影响。

鉴于现有视觉异常生成方法的局限性，我们旨在利用最少的正常数据（无需任何异常样本）生成多样且逼真的未知异常。这一目标促使我们探索 Stable
Diffusion（SD）\[27\]—— 一种潜在文本到图像扩散模型，其以跨多种领域生成多样化图像而闻名。尽管 SD
展现出令人印象深刻的图像生成能力，但其并非专门为视觉异常生成设计。因此，当直接用于异常生成时，SD
可能会产生偏离预期正态分布的图像，无法准确呈现真实异常模式（图 3（b））。先前方法 \[17,35\] 建议在可用正常或异常样本上微调
SD，但这种方法在数据稀缺场景中受到限制，且可能削弱 SD 对未知数据和异常类型的泛化能力。

在这项工作中，我们提出了Anomaly Anything（AnomalyAny）框架，用于生成逼真且多样化的未知视觉异常。我们利用Stable
Diffusion（SD）模型结合异常文本描述来生成未知视觉异常，这些文本描述可手动定义，或通过GPT-4\[24\]以物体类别为输入自动获取。不同于在正常数据上微调SD模型，我们引入了**测试时正常样本条件化**方法——通过单个正常样本引导生成过程。该方法保留了SD的多样性和泛化能力，使其能够针对新数据和新型异常类型生成异常样本。仅需一个模型，我们就能根据任意异常描述为任何新物体生成逼真且多样的异常样本。

我们发现，原始SD模型难以生成真实异常样本，主要受两个固有挑战制约：其一，SD训练数据中异常样本相对稀缺；其二，与常见物体和图案不同，异常图案通常仅占据图像的小区域，在生成过程中易被忽略。为解决这些问题，我们提出**注意力引导的异常优化方法**，通过最大化与异常标记相关的注意力值，强制模型聚焦于生成异常概念。为进一步引导和细化生成结果，我们还提出**提示引导的异常细化方法**，利用更详细的异常描述作为额外语义指导。我们的主要贡献包括：

  - 我们提出了AnomalyAny，这是一个用于生成未知异常的视觉异常生成框架。用户只需提供物体的任意正常图像和异常描述，即可生成逼真且多样化的异常样本。
  - 我们引入了**注意力引导的异常优化方法和提示引导的异常细化方法**，以克服Stable
    Diffusion（SD）在异常生成中的局限性。我们提出的方法无需额外训练或大量正常/异常数据样本，即可获得比现有方法更真实的生成结果。
  - 我们验证了所提出的AnomalyAny在生成质量和辅助下游异常检测模型训练两方面的有效性。

## 2 相关工作

### 2.1 异常生成

异常数据的稀缺性促使众多研究致力于合成异常样本\[10\]。一种方法是通过从测试集中的少量异常样本\[21,30\]、外部数据集的自然图案\[7,33\]或直接从正常图像本身\[19,29\]**裁剪粘贴**异常图案来增强正常训练样本。尽管这些方法简单有效，但生成的样本缺乏真实性和多样性。另一种方法采用**生成模型**（如生成对抗网络（GAN））来合成异常\[11,34\]。近年来，扩散模型\[14,17,31,35\]的发展使通过在异常数据上微调模型来生成更多样化、更逼真异常的方法成为可能。然而，这些方法通常需要大量正常和/或异常数据样本来学习特定数据集的分布，这在数据有限的场景中往往不切实际。此外，这些方法只能生成与训练集相似的样本（即已知异常），无法生成未知异常。但在实际应用中，假设已知所有可能的异常类型往往不现实，这凸显了对能够泛化到未知类型的方法的需求。为了生成未知但逼真的异常，AnomalyAny直接部署预训练的SD模型进行异常合成，无需额外训练，为跨任意物体的高质量异常生成提供了更灵活的解决方案。

### 2.2 异常检测

可用异常样本的稀缺性使得无监督异常检测（AD）\[4,9,28\]成为该领域的主流范式，其核心是对正常分布建模并将异常识别为离群值\[4,9,28\]。但这类方法依赖于足够数量的正常样本来充分表征潜在分布——而产品变体与工业配置的多样性导致代表性正常样本匮乏，促使小样本异常检测（few-shot
AD）\[22,32\]受到越来越多关注。现有小样本方法常借助CLIP\[25\]等在大规模数据集预训练的视觉-语言模型获取额外知识，通过计算数据样本与正常/异常文本提示的相似度实现检测\[13,18,20\]。然而，仅用正常样本训练AD模型仍面临关键挑战：模型缺乏对异常分布的认知，这凸显了借助逼真异常样本提升检测性能的必要性。

## 3\. 预备知识：Stable Diffusion

扩散模型。去噪扩散概率模型（DDPM）\[15\]通过定义长度为T的马尔可夫链来学习目标数据分布。前向过程中，该链逐步向给定数据样本
$x\_0$ 添加噪声，得到含噪样本序列 $x\_t$ （ $t \\in T$ ）。反向过程中，学习一个由θ参数化的模型
$\\epsilon\_{\\theta}$ ，用于预测每个时间步t添加的噪声。我们的方法基于Stable
Diffusion（SD）\[27\]，这是一种潜在扩散模型（LDM），它将去噪扩散过程应用于变分自编码器（VAE）潜在空间中x的潜在表示
$z$ ，其学习目标为预测每个时间步t添加的噪声，表达式为：

$$L\_{L D M}:=\\mathbb{E}*{\\mathcal{E}(x), \\epsilon \\sim
\\mathcal{N}(0,1), t}\\left\[\\left|
\\epsilon-\\epsilon*{\\theta}\\left(z\_{t}, t\\right)\\right|
\_{2}^{2}\\right\], \\quad(1)$$

其中 $z\_t$ 表示时间步t的含噪潜在表示。推理时，反向过程从随机噪声 $x\_T \\sim N(0, I)$
开始，按时间步从T到0逐步从噪声中生成图像样本。

文本条件。SD通过交叉注意力机制引入文本引导。SD潜在空间中的去噪UNet网络由自注意力层和分辨率为 $P \\in(64,32,16,8)$
的交叉注意力层组成。给定由N个token组成的文本提示c，通过CLIP文本编码器T\[26\]获得引导向量 $\\tau(c)$ 。随后，
$\\tau(c)$ 通过每个交叉注意力层映射到DDPM模型 $\\epsilon\_{\\theta}$ 的中间特征图，具体如下：

$$A = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d}}\\right) \\cdot V,
\\quad A \\in \[P, P, N\],$$

$$Q = W\_Q^{(i)} \\cdot \\varphi\_i(z\_t), \\quad K = W\_K^{(i)} \\cdot
\\tau(c), \\quad V = W\_V^{(i)} \\cdot \\tau(c), \\quad (3)$$

其中 $\\varphi\_i(z\_t)$ 是UNet的中间特征，A、Q、K、V分别表示注意力矩阵、查询矩阵、键矩阵和值矩阵。
$W^{(i)}$ 表示可学习的投影矩阵。 在图像-文本对 ${x, c}$ 上，文本条件扩散模型可通过以下目标函数进行学习：

$$L\_{L D M}:=\\mathbb{E}*{\\mathcal{E}(x), c, \\epsilon \\sim
\\mathcal{N}(0,1), t}\\left\[\\left|
\\epsilon-\\epsilon*{\\theta}\\left(z\_{t}, t, \\tau(c)\\right)\\right|
\_{2}^{2}\\right\] . \\ (4)$$

## 4 方法

在这项工作中，我们提出了AnomalyAny——一种用于在未知物体和异常类型上生成逼真的、可通过提示控制的异常的新型框架，如图2所示。该框架包含三个核心模块：首先通过测试时正常样本条件化（4.1节）基于单个正常样本引导生成过程，实现对新物体和异常类型的异常生成；生成阶段引入两阶段优化流程，注意力引导的异常优化（4.2节）先将生成焦点集中于复杂异常概念，再通过提示引导的异常细化（4.3节）利用详细文本描述进一步提升生成质量。

### 4.1. 测试时正常样本条件化

在这项工作中，我们利用Stable
Diffusion（SD）生成未知视觉异常。尽管SD在图像生成上表现出色，但其潜在分布的固有多样性会导致生成图像与特定异常检测（AD）数据集中的正常图像存在显著差异（如图3（b）所示）。为生成与目标正常分布一致的图像，我们未对SD进行微调，而是在生成过程中直接融入正常样本信息：给定正常样本
$x^{normal}$ 及其由变分自编码器（VAE）编码器得到的潜在表示 $z^{normal}=E(x^{normal})$
，通过DDPM噪声调度器控制逐步添加噪声，得到样本序列 $z\_{1}^{normal}$ 、
$z\_{2}^{normal}$ 、…、 $z\_{T}^{normal}$ 。推理过程不从头开始于 $z\_{T} \\sim N(0,
I)$ ，而是从 $t\_{start} = T \\cdot (1-\\gamma)$ 步骤开始，利用含噪潜在表示
$z\_{t\_{start}}^{normal}$ 传递来自引导正常图像 $x^{normal}$
的损坏特征，从而在测试时无需额外训练即可为新物体类型生成异常。参数γ控制起始步骤（即添加到引导数据样本的噪声尺度），实验中设
$\\gamma=0.25$ 以平衡与原始分布的相似度和促进多样化结果的推理步骤。

为了基于给定正常样本实现更精确的控制，我们将掩码输入作为可选约束，用于指定生成异常的位置。该掩码既可以作为前景掩码，将异常限制在目标物体上，也可以作为预定义的详细掩码，精确界定异常区域。对于时间步t的预测潜在表示
$z\_t$ ，我们执行以下操作：

$$z\_{t}=mask \\odot z\_{t}+(1- mask ) \\odot z\_{t}^{normal} . \\ (5)$$

这种公式化方法在掩码区域之外保留了正常引导样本的分布，确保修改仅局限于指定区域。

### 4.2. 注意力引导的异常优化

我们通过提供异常文本描述（如“一张损坏的\[cls\]（如瓶子）的照片”）来生成特定物体类型（标记为\[cls\]）的异常样本。如前所述，由于异常生成的复杂性，生成图像往往会忽略所需的异常语义（如图3（d）所示）。为解决这一挑战，我们引入注意力引导的异常优化机制，以强制生成这些关键但难以捕捉的异常概念。首先，我们聚合SD的注意力图用于后续优化：给定由N个token组成的文本提示c，在每个推理步骤t，获取与c对应的注意力图集合
$A\_t \\in P×P×N$ ，其中 $A\[:,:,i\]$ 表示分配给token $c\_i$
的概率。遵循文献\[6\]，对16×16分辨率的注意力图取平均（该分辨率语义信息最强），生成的
$\\bar{A}\_t$ 在token维度归一化并通过高斯函数平滑：

$$\\overline{A}*{t}=\\text{Gaussian}\\left(\\text{softmax}\\left(\\overline{A}*{t}\\right)\\right),
\\overline{A}\_{t} \\in 16 × 16 × N$$

在生成过程中，通过 $\\bar{A}*{t}$ 引导的优化强制图像传达异常类型token $c\_j$
（如“damaged”）的语义：每个时间步t，最大化与异常描述相关的注意力以优化中间潜在表示
$z\_t$ ，优化后去噪得到 $z*{t-1}$ （如图2所示）。具体而言，提取与 $c\_j$ 相关的注意力图
$\\bar{A}*{t}^{j}$ ，通过损失函数 $L*{att}$ 计算 $z\_t$
的梯度更新，并允许通过掩码限制优化区域。步骤t的优化公式为：

$$\\mathcal{L}*{att} = 1 - \\max(\\overline{A}*{t}^{j} \\odot
\\text{mask}), \\quad z\_{t} \\leftarrow z\_{t} - \\alpha\_{t} \\cdot
\\nabla\_{z\_{t}} \\mathcal{L}\_{att} \\odot \\text{mask}$$

其中 $\\alpha\_t$ 为步长，该目标通过最大化 $\\bar{A}\_{t}^{j}$
强化异常token的激活。通过迭代优化，确保生成图像准确表达目标异常语义。
